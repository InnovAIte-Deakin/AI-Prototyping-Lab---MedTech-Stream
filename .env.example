FRONTEND_URL=http://localhost:3000
NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
# Never commit real secrets. Set this in your local .env only; do not source this file.
# Example (uncomment and put in .env):
# OPENAI_API_KEY=sk-your-key-here

# Recommended default model with broad access and good speed
OPENAI_MODEL=o4-mini

# OpenAI endpoints
OPENAI_API_BASE=https://api.openai.com/v1
# Optional alternate name supported by backend:
# OPENAI_BASE_URL=https://api.openai.com/v1

# Host and security
ALLOWED_HOSTS=localhost,127.0.0.1,testserver,backend,frontend
ENABLE_OCR=1

# LLM tuning
OPENAI_REASONING_EFFORT=high
# Single limit used for both Responses (max_output_tokens) and Chat (max_completion_tokens)
OPENAI_MAX_OUTPUT_TOKENS=1600
OPENAI_TIMEOUT_S=60
# Prefer Chat Completions for o*-family models
OPENAI_USE_RESPONSES=0
